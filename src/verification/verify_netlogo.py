"""
NetLogo Code Verifier Module

This module provides functionality to validate and verify NetLogo code generated by LLMs.
It ensures the code is syntactically correct, safe to execute, and follows best practices
for movement rules that agents use to explore the environment.

Dependencies:
- Python 3.8+
- Standard library modules: re, typing
"""

import re
from typing import List, Tuple, Set, Dict, Optional, Union, Pattern, Iterator, NamedTuple
from dataclasses import dataclass
from enum import Enum, auto
import logging

logger = logging.getLogger(__name__)


# --- Tokenizer Components ---

class TokenType(Enum):
    COMMAND = auto()
    REPORTER = auto()
    VARIABLE = auto()
    NUMBER = auto()
    STRING_LITERAL = auto()
    OPERATOR = auto()        # Arithmetic: +, -, *, /, ^
    COMPARISON = auto()      # Comparison: =, !=, >, <, >=, <=
    LOGICAL = auto()         # Logical: and, or, not
    LPAREN = auto()          # (
    RPAREN = auto()          # )
    LBRACKET = auto()        # [
    RBRACKET = auto()        # ]
    COMMENT = auto()         # ; ...
    WHITESPACE = auto()      # Spaces, tabs
    NEWLINE = auto()         # \n
    IDENTIFIER = auto()      # General identifier before classification
    UNKNOWN = auto()         # Unrecognized token
    EOF = auto()             # End of File/Input

@dataclass
class Token:
    type: TokenType
    value: str
    line: int
    column: int

# --- End Tokenizer Components ---

class ErrorSeverity(Enum):
    """Severity levels for validation errors."""
    WARNING = "warning"
    ERROR = "error"

@dataclass
class ValidationError:
    """Class to represent validation errors with context."""
    message: str
    line_number: Optional[int] = None
    code_snippet: Optional[str] = None
    severity: ErrorSeverity = ErrorSeverity.ERROR

    def __str__(self) -> str:
        """String representation of the validation error."""
        location = f" at line {self.line_number}" if self.line_number is not None else ""
        snippet = f"\n  Code: '{self.code_snippet}'" if self.code_snippet else ""
        return f"{self.severity.value.upper()}{location}: {self.message}{snippet}"

@dataclass
class ValidationResult:
    """Result of a validation check."""
    is_valid: bool
    errors: List[ValidationError] = None

    def __init__(self, is_valid: bool, errors: Optional[List[ValidationError]] = None):
        self.is_valid = is_valid
        self.errors = errors or []

    def add_error(self, error: ValidationError) -> None:
        """Add an error to the result."""
        self.errors.append(error)
        self.is_valid = False

    def merge(self, other: 'ValidationResult') -> None:
        """Merge another validation result into this one."""
        self.is_valid = self.is_valid and other.is_valid
        self.errors.extend(other.errors)

class CodeComplexity(Enum):
    """Complexity levels for NetLogo code."""
    SIMPLE = 1      # Basic movement without conditions
    BASIC = 2       # Simple conditionals
    MODERATE = 3    # Multiple conditions, basic sensing
    ADVANCED = 4    # Complex conditions, environment awareness
    COMPLEX = 5     # Advanced sensing, memory usage
    SOPHISTICATED = 6  # Multiple strategies, adaptation
    EXPERT = 7      # Optimal pathfinding, complex decision making

class NetLogoVerifier:
    """
    NetLogo Code Verification and Validation Class
    
    This class provides comprehensive validation for NetLogo code generated by LLMs,
    focusing on verifying movement rules that agents use to explore environments.
    
    Public Methods:
    - is_safe(code: str) -> Tuple[bool, str]: Main validation method
    - validate(code: str) -> ValidationResult: Detailed validation with multiple errors
    - measure_complexity(code: str) -> CodeComplexity: Measures code complexity
    """
    def __init__(self, config: Optional[Dict] = None):
        """
        Initialize verifier with optional configuration.
        
        Args:
            config: Optional dictionary with configuration parameters
        """
        self.config = config or {}
        
        # Configure limits
        self.max_code_length = self.config.get("max_code_length", 10000)
        self.max_value = self.config.get("max_value", 1000)
        self.min_value = self.config.get("min_value", -1000)
        
        # Allowed NetLogo primitives
        self.allowed_commands = {
            # Movement commands
            'fd', 'forward',
            'rt', 'right',
            'lt', 'left',
            'bk', 'back',
            # Control structures
            'if', 'ifelse', 'ifelse-value',
            # Variable operations
            'set', 'let'
        }
        
        self.allowed_reporters = {
            # Random generators
            'random', 'random-float',
            # Math functions
            'sin', 'cos', 'tan',
            # List operations
            'item', 'count', 'length', 'position',
            # Agent properties
            'xcor', 'ycor', 'heading',
            # Agent sensing
            'any?', 'in-radius', 'distance', 'towards',
            # Logic
            'and', 'or', 'not'
        }
        
        self.dangerous_primitives = {
            # Agent lifecycle
            'die', 'kill', 'create', 'hatch', 'sprout',
            # Agent control
            'ask', 'of', 'with',
            # Code execution
            'run', 'runresult',
            # File operations
            'file', 'import', 'export',
            # External code
            'python', 'js',
            # Simulation control
            'clear', 'reset', 'setup', 'go',
            # Loops
            'while', 'loop', 'repeat', 'forever',
            # Breeds
            'breed', 'create-ordered',
            # Network/extension operations
            'hubnet', 'gis', 'sql',
            # System operations
            'wait', 'beep', 'system',
            # Global state
            'clear-all', 'reset-ticks'
        }
        
        self.arithmetic_operators = {'+', '-', '*', '/', '^'}
        self.comparison_operators = {'=', '!=', '>', '<', '>=', '<='}
        self.allowed_variables = {
            'input', 'energy', 'lifetime', 'food-collected',
            'xcor', 'ycor', 'heading', 'who', 'input-resource-distances', 'input-resource-types',
            'food-observations', 'poison-observations', '"silver"', '"gold"', '"crystal"',
            'weight' # Added based on analysis of storeprompts.py
        }
        
        # Precompile regex patterns for performance
        self._compile_regex_patterns()
        # Compile tokenizer patterns
        self._compile_tokenizer_patterns()

    def _compile_regex_patterns(self) -> None:
        """Precompile regex patterns for validation (kept for now, may be deprecated)."""
        # Keep number pattern for value range checks (using regex temporarily)
        self.number_pattern = re.compile(r'[+-]?\d+(\.\d+)?([eE][+-]?\d+)?')
        # Old expression patterns removed
        self.comment_pattern = re.compile(r';.*$', re.MULTILINE) # Still useful

    def _compile_tokenizer_patterns(self) -> None:
        """Compile regex patterns for the tokenizer."""
        # Order matters! More specific patterns first.
        token_specs = [
            ('COMMENT',        r';[^\n]*'),                  # Comment until newline
            ('NEWLINE',        r'\n'),                       # Newline
            ('WHITESPACE',     r'[ \t]+'),                   # Whitespace (excluding newline)
            ('NUMBER',         r'[+-]?\d+(\.\d+)?([eE][+-]?\d+)?'), # Numbers (int, float, sci)
            ('STRING_LITERAL', r'"(?:\\.|[^"\\])*"'),        # String literal in double quotes
            ('LPAREN',         r'\('),                       # Left parenthesis
            ('RPAREN',         r'\)'),                       # Right parenthesis
            ('LBRACKET',       r'\['),                       # Left bracket
            ('RBRACKET',       r'\]'),                       # Right bracket
            ('COMPARISON',     r'!=|>=|<=|=|>|<'),           # Comparison operators
            ('OPERATOR',       r'[+\-*/^]'),                 # Arithmetic operators
            # Identifier needs to be broad, classification happens later
            ('IDENTIFIER',     r'[a-zA-Z][\w\-]*\??'),       # Identifier (letters, digits, -, ?, starting with letter)
            ('UNKNOWN',        r'.'),                        # Any other character
        ]
        # Combine into a single regex for efficiency
        self.tokenizer_regex = re.compile(
            '|'.join(f'(?P<{name}>{pattern})' for name, pattern in token_specs)
        )

    def _tokenize(self, code: str) -> Iterator[Token]:
        """
        Generates a stream of tokens from the input NetLogo code string.

        Args:
            code: The raw NetLogo code string.

        Yields:
            Token objects representing the lexical elements of the code.
        """
        line_num = 1
        line_start = 0
        for mo in self.tokenizer_regex.finditer(code):
            kind = mo.lastgroup
            value = mo.group()
            column = mo.start() - line_start + 1

            if kind == 'NEWLINE':
                yield Token(TokenType.NEWLINE, value, line_num, column)
                line_num += 1
                line_start = mo.end()
            elif kind == 'WHITESPACE':
                pass # Ignore whitespace
            elif kind == 'COMMENT':
                pass # Ignore comments
            elif kind == 'UNKNOWN':
                yield Token(TokenType.UNKNOWN, value, line_num, column)
            else:
                token_type = TokenType[kind] # Map regex group name to Enum
                # Further classify IDENTIFIERs based on known lists
                if token_type == TokenType.IDENTIFIER:
                    val_lower = value.lower()
                    if val_lower in self.allowed_commands:
                        token_type = TokenType.COMMAND
                    elif val_lower in self.allowed_reporters:
                        token_type = TokenType.REPORTER
                    elif val_lower in {'and', 'or', 'not'}:
                         token_type = TokenType.LOGICAL
                    # Note: Variables remain IDENTIFIER unless matched above.

                yield Token(token_type, value, line_num, column)

        # Yield EOF token at the end
        yield Token(TokenType.EOF, '', line_num, len(code) - line_start + 1)


    def is_safe(self, code: str) -> Tuple[bool, str]:
        """
        Simplified interface to validate NetLogo code for safety and correctness.
        """
        result = self.validate(code)
        if not result.is_valid:
            return False, "\n".join(str(error) for error in result.errors)
        return True, "Code appears safe"

    def validate(self, code: str) -> ValidationResult:
        """
        Comprehensive validation of NetLogo code with detailed error reporting.
        """
        result = ValidationResult(True)

        # --- Step 1: Tokenization ---
        tokens = list(self._tokenize(code))
        filtered_tokens = [t for t in tokens if t.type not in {TokenType.WHITESPACE, TokenType.COMMENT, TokenType.NEWLINE} or t.type == TokenType.EOF]

        if not filtered_tokens or all(t.type == TokenType.EOF for t in filtered_tokens):
             result.add_error(ValidationError("Empty code or only comments/whitespace"))
             return result

        # Check for unknown tokens
        for token in filtered_tokens:
            if token.type == TokenType.UNKNOWN:
                result.add_error(ValidationError(
                    f"Unknown token: '{token.value}'",
                    line_number=token.line,
                    code_snippet=code.splitlines()[token.line-1][max(0, token.column-10):token.column+9]
                ))
        if not result.is_valid:
             return result

        # --- Step 2: Code Length Check ---
        if len(code) > self.max_code_length:
            result.add_error(ValidationError(f"Code exceeds maximum length of {self.max_code_length} characters"))

        # --- Step 3: Basic Structural Validation ---
        dangerous_result = self._check_dangerous_primitives_tokenized(filtered_tokens)
        result.merge(dangerous_result)
        if not result.is_valid: return result

        balance_result = self._check_brackets_balance_tokenized(filtered_tokens)
        result.merge(balance_result)
        if not result.is_valid: return result

        # --- Step 4: Detailed Validation ---
        movement_result = self._check_movement_commands_tokenized(filtered_tokens)
        result.merge(movement_result)

        syntax_result = self._check_syntax_tokenized(filtered_tokens)
        result.merge(syntax_result)
        # Stop early if major syntax errors occurred before checking ranges
        if not result.is_valid: return result

        # Call the token-based value range check
        range_result = self._check_value_ranges(filtered_tokens)
        result.merge(range_result)

        return result

    def _check_dangerous_primitives_tokenized(self, tokens: List[Token]) -> ValidationResult:
        """Validate against dangerous primitives using tokens."""
        result = ValidationResult(True)
        for token in tokens:
            # Check commands, reporters, and general identifiers that might match
            if token.type in {TokenType.COMMAND, TokenType.REPORTER, TokenType.IDENTIFIER}:
                if token.value.lower() in self.dangerous_primitives:
                    result.add_error(ValidationError(
                        f"Dangerous primitive found: {token.value}",
                        line_number=token.line,
                        code_snippet=token.value
                    ))
        return result

    def _check_brackets_balance_tokenized(self, tokens: List[Token]) -> ValidationResult:
        """Validate bracket/parenthesis balance using tokens."""
        result = ValidationResult(True)
        stack = []
        bracket_map = {TokenType.LPAREN: TokenType.RPAREN, TokenType.LBRACKET: TokenType.RBRACKET}
        opening_types = {TokenType.LPAREN, TokenType.LBRACKET}
        closing_types = {TokenType.RPAREN, TokenType.RBRACKET}

        for token in tokens:
            if token.type in opening_types:
                stack.append((bracket_map[token.type], token))
            elif token.type in closing_types:
                if not stack:
                    result.add_error(ValidationError(
                        f"Unmatched closing bracket/parenthesis: '{token.value}'",
                        line_number=token.line, code_snippet=token.value
                    ))
                else:
                    expected_type, opening_token = stack.pop()
                    if token.type != expected_type:
                        result.add_error(ValidationError(
                            f"Mismatched bracket/parenthesis: Expected closing for '{opening_token.value}' (line {opening_token.line}) but found '{token.value}'",
                            line_number=token.line, code_snippet=f"...{opening_token.value}...{token.value}..."
                        ))

        for _, opening_token in stack:
             result.add_error(ValidationError(
                 f"Unclosed bracket/parenthesis: '{opening_token.value}'",
                 line_number=opening_token.line, code_snippet=opening_token.value
             ))
        return result

    def _check_movement_commands_tokenized(self, tokens: List[Token]) -> ValidationResult:
        """Verify presence of at least one movement command using tokens."""
        result = ValidationResult(True)
        has_ifelse_value = any(t.type == TokenType.COMMAND and t.value.lower() == 'ifelse-value' for t in tokens)
        if has_ifelse_value:
            return result # ifelse-value doesn't require movement commands

        movement_commands = {'fd', 'forward', 'rt', 'right', 'lt', 'left', 'bk', 'back'}
        found_movement = any(t.type == TokenType.COMMAND and t.value.lower() in movement_commands for t in tokens)

        if not found_movement:
            result.add_error(ValidationError(
                "No movement commands found. Code must include at least one movement command: fd, rt, lt, or bk",
                severity=ErrorSeverity.ERROR
            ))
        return result

    def _check_syntax_tokenized(self, tokens: List[Token]) -> ValidationResult:
        """Validate overall syntax using tokens (main entry point)."""
        result = ValidationResult(True)
        i = 0
        while i < len(tokens) and tokens[i].type != TokenType.EOF:
            token = tokens[i]
            consumed_count = 1 # Default consumption

            if token.type == TokenType.COMMAND:
                if token.value.lower() in {'if', 'ifelse', 'ifelse-value'}:
                    if_result, consumed_count = self._validate_if_statement(tokens, i)
                    result.merge(if_result)
                elif token.value.lower() in self.allowed_commands:
                    cmd_result, consumed_count = self._validate_command(tokens, i)
                    result.merge(cmd_result)
                else:
                    # Should have been caught by dangerous check, but safeguard
                    result.add_error(ValidationError(f"Unexpected command: {token.value}", line_number=token.line))
            elif token.type in {TokenType.RPAREN, TokenType.RBRACKET}:
                 # Closing brackets/parens shouldn't appear at the top level
                 result.add_error(ValidationError(f"Unexpected closing token: '{token.value}'", line_number=token.line))
            # Handle multi-conditional ifelse starting with '('
            elif token.type == TokenType.LPAREN:
                 # Peek ahead: Expect 'ifelse' or 'ifelse-value' command next
                 if i + 1 < len(tokens) and tokens[i+1].type == TokenType.COMMAND and tokens[i+1].value.lower() in {'ifelse', 'ifelse-value'}:
                      statement_type = tokens[i+1].value.lower()
                      # Call the multi-conditional validator starting from the LPAREN
                      multi_cond_result, consumed_count = self._validate_multi_conditional(tokens, i, statement_type)
                      result.merge(multi_cond_result)
                 else:
                      # Parenthesized expression not allowed at top level, or invalid multi-conditional start
                      peek_token_desc = f"'{tokens[i+1].value}' ({tokens[i+1].type.name})" if i + 1 < len(tokens) else "end of input"
                      result.add_error(ValidationError(f"Unexpected parenthesis '(' at top level, or invalid start to multi-conditional ifelse (found {peek_token_desc} after '(')", line_number=token.line))
                      # Try to consume until matching RPAREN for basic recovery
                      paren_level = 1
                      consumed_count = 1
                      temp_i = i + 1
                      while temp_i < len(tokens):
                           consumed_count += 1
                           if tokens[temp_i].type == TokenType.LPAREN: paren_level += 1
                           elif tokens[temp_i].type == TokenType.RPAREN:
                               paren_level -= 1
                               if paren_level == 0: break
                           elif tokens[temp_i].type == TokenType.EOF: break
                           temp_i += 1
                      if paren_level != 0:
                           result.add_error(ValidationError(f"Unclosed parenthesis starting on line {token.line}", line_number=token.line))


            # Other tokens are unexpected at the top level
            else:
                 result.add_error(ValidationError(f"Unexpected token at top level: {token.type.name} ('{token.value}')", line_number=token.line))
                 consumed_count = 1 # Consume the unexpected token

            i += consumed_count # Advance by the number of tokens consumed by the validator
            if not result.is_valid and consumed_count > 0 : # Check consumed_count > 0 to avoid infinite loop if validator returns 0
                 # If a top-level command failed validation, stop further checks at this level
                 # to avoid cascading errors from a single malformed command.
                 # However, allow checking subsequent independent commands.
                 # Consider if we should stop entirely on first error. For now, continue.
                 pass

        return result

    # --- Expression Validator (Pratt Parser Style) ---

    # Operator precedence levels (higher value = higher precedence)
    # Using common arithmetic/logical precedence. NetLogo might have nuances.
    OPERATOR_PRECEDENCE = {
        # Arithmetic (PEMDAS/BODMAS like)
        '^': 4, # Exponentiation (Right-associative, handle later if needed)
        '*': 3, '/': 3, # Multiplication/Division
        '+': 2, '-': 2, # Addition/Subtraction
        # Comparison
        '=': 1, '!=': 1, '>': 1, '<': 1, '>=': 1, '<=': 1,
        # Logical
        'not': 5, # Unary logical NOT (high precedence)
        'and': 0,
        'or': -1, # Lowest precedence
        # Parentheses are handled by structure, not precedence map
        # Unary +/- need special handling in prefix parsing
    }

    # Define expected number of arguments for reporters
    REPORTER_ARITY = {
        # Random
        'random': 1, 'random-float': 1,
        # Math
        'sin': 1, 'cos': 1, 'tan': 1,
        # List (Note: 'item' takes list and index)
        'item': 2, 'count': 1, 'length': 1, 'position': 2,
        # Agent properties (zero args)
        'xcor': 0, 'ycor': 0, 'heading': 0,
        # Agent Sensing (Treat agentsets/targets as single expressions for now)
        # TODO: Refine validation for agentset/agent/patch argument types if needed.
        'any?': 1,
        'in-radius': 2, # agentset, radius
        'distance': 1, # target
        'towards': 1, # target
        # Logical operators 'and', 'or', 'not' are handled by precedence parsing, not here.
    }


    def _get_token_precedence(self, token: Optional[Token]) -> int:
        """Returns the precedence of an infix operator token, or -2 if not an infix operator."""
        if token is None:
            return -2
        # Check arithmetic/comparison operators first
        if token.type in {TokenType.OPERATOR, TokenType.COMPARISON}:
            return self.OPERATOR_PRECEDENCE.get(token.value, -2)
        # Check logical operators (only 'and', 'or' are infix)
        if token.type == TokenType.LOGICAL and token.value.lower() in {'and', 'or'}:
             return self.OPERATOR_PRECEDENCE.get(token.value.lower(), -2)
        return -2 # Not a recognized infix operator

    def _can_start_expression(self, token: Optional[Token]) -> bool:
        """Checks if a token type can potentially start a valid expression."""
        if token is None:
            return False
        # Primary terms
        if token.type in {TokenType.NUMBER, TokenType.IDENTIFIER, TokenType.STRING_LITERAL,
                          TokenType.LPAREN, TokenType.REPORTER}:
            return True
        # Prefix operators
        if token.type == TokenType.OPERATOR and token.value in {'+', '-'}:
            return True
        if token.type == TokenType.LOGICAL and token.value.lower() == 'not':
            return True
        return False

    def _parse_prefix_or_primary(self, tokens: List[Token], start_index: int) -> Tuple[ValidationResult, int]:
        """
        Parses prefix operators (unary -, +, not) and primary expression terms
        (numbers, variables, strings, parenthesized expressions, reporter calls).
        This is the first part of the Pratt parser logic.
        Operator precedence and reporter calls are TODOs.

        Args:
            tokens: The list of tokens.
            start_index: The index in the token list where the prefix/primary starts.

        Returns:
            A tuple containing:
            - ValidationResult: Indicates if the parsed prefix/primary is valid.
            - int: The index of the token immediately *after* the parsed part.
        """
        result = ValidationResult(True)
        i = start_index

        if i >= len(tokens) or tokens[i].type == TokenType.EOF:
            result.add_error(ValidationError(
                "Expected expression term or prefix operator, found end of input",
                line_number=tokens[i-1].line if i > 0 else 1
            ))
            return result, i # Consumes nothing

        current_token = tokens[i]
        next_index = i + 1 # Default consumption is 1 token

        # --- Handle Prefix Operators ---
        # Check for unary minus/plus (distinct from binary operators)
        if current_token.type == TokenType.OPERATOR and current_token.value in {'+', '-'}:
            # Treat as unary prefix operator
            op_token = current_token
            # Recursively parse the operand that follows the unary operator.
            # The precedence for the operand parsing doesn't strictly matter here,
            # but using a high value ensures it binds tightly. Let's use 5.
            operand_result, operand_end_index = self._validate_expression(tokens, i + 1, min_precedence=5) # High precedence for operand
            if not operand_result.is_valid:
                result.merge(operand_result)
                if result.errors:
                    result.errors[-1].message = f"Invalid operand for unary '{op_token.value}': {result.errors[-1].message}"
            # The result is valid if the operand was valid.
            next_index = operand_end_index
            # No specific validation needed for the unary op itself, just its operand.
            return result, next_index # Return after handling prefix op + operand

        # Check for logical 'not'
        if current_token.type == TokenType.LOGICAL and current_token.value.lower() == 'not':
            op_token = current_token
            not_precedence = self.OPERATOR_PRECEDENCE.get('not', 5)
            operand_result, operand_end_index = self._validate_expression(tokens, i + 1, min_precedence=not_precedence)
            if not operand_result.is_valid:
                result.merge(operand_result)
                if result.errors:
                    result.errors[-1].message = f"Invalid operand for '{op_token.value}': {result.errors[-1].message}"
            next_index = operand_end_index
            return result, next_index # Return after handling prefix op + operand


        # --- Handle Primary Terms (if not a prefix operator) ---
        if current_token.type == TokenType.NUMBER:
            pass # Valid primary term
        elif current_token.type == TokenType.IDENTIFIER:
            if current_token.value.lower() not in self.allowed_variables:
                result.add_error(ValidationError(
                    f"Unknown or disallowed identifier/variable: '{current_token.value}'",
                    line_number=current_token.line, code_snippet=current_token.value
                ))
            # Valid variable is a primary term
        elif current_token.type == TokenType.STRING_LITERAL:
            if current_token.value.lower() not in self.allowed_variables:
                 result.add_error(ValidationError(
                     f"Disallowed string literal used as value: {current_token.value}",
                     line_number=current_token.line, code_snippet=current_token.value
                 ))
            # Valid allowed string is a primary term

        # --- Handle Parentheses ---
        elif current_token.type == TokenType.LPAREN:
            # Parse the expression inside parentheses, starting with lowest operator precedence.
            inner_result, inner_end_index = self._validate_expression(tokens, i + 1, min_precedence=-1) # Use -1 base precedence
            if not inner_result.is_valid:
                result.merge(inner_result)
                next_index = inner_end_index # Advance past invalid inner part
            else:
                # Check for the closing parenthesis.
                if inner_end_index < len(tokens) and tokens[inner_end_index].type == TokenType.RPAREN:
                    next_index = inner_end_index + 1 # Consume the RPAREN
                else:
                    expected_line = tokens[inner_end_index - 1].line if inner_end_index > 0 else current_token.line
                    result.add_error(ValidationError(
                        f"Expected ')' to close parenthesis opened on line {current_token.line}",
                        line_number=expected_line
                    ))
                    next_index = inner_end_index # Don't consume potentially missing RPAREN

        # --- Handle Reporters ---
        elif current_token.type == TokenType.REPORTER:
            reporter_name = current_token.value.lower()
            arity = self.REPORTER_ARITY.get(reporter_name)

            if arity is None:
                # This case should ideally be caught by the tokenizer or allowed_reporters check,
                # but serves as a safeguard within the expression parser.
                result.add_error(ValidationError(
                    f"Unknown or disallowed reporter '{current_token.value}' used in expression",
                    line_number=current_token.line, code_snippet=current_token.value
                ))
                # Consume the token to avoid loops, but mark as invalid.
                next_index = i + 1
            else:
                # Consume the reporter token itself
                current_arg_index = i + 1
                # Parse the expected number of arguments
                for arg_num in range(arity):
                    if current_arg_index >= len(tokens) or tokens[current_arg_index].type == TokenType.EOF:
                        result.add_error(ValidationError(
                            f"Expected argument {arg_num + 1} for reporter '{reporter_name}', but found end of input",
                            line_number=current_token.line
                        ))
                        current_arg_index = len(tokens) # Move index to end
                        break # Stop parsing args for this reporter

                    # Parse the argument expression recursively using the main validator.
                    # Start with lowest operator precedence for each argument.
                    arg_result, arg_end_index = self._validate_expression(tokens, current_arg_index, min_precedence=-1) # Use -1 base precedence

                    if not arg_result.is_valid:
                        result.merge(arg_result)
                        # Add context to the *first* error reported for this argument
                        if result.errors:
                            # Find the most recent error added by the recursive call and prepend context
                            # This assumes merge adds errors to the end.
                            result.errors[-1].message = f"Invalid argument {arg_num + 1} for '{reporter_name}': {result.errors[-1].message}"
                        # Continue parsing subsequent args even if one fails, to catch more structural errors,
                        # but the overall result for the reporter call is now invalid.

                    # Move to the start of the next potential argument
                    current_arg_index = arg_end_index

                # After parsing all expected arguments, the next_index is where the reporter call ends.
                next_index = current_arg_index

        # --- Handle Unexpected Tokens ---
        else:
            result.add_error(ValidationError(
                f"Unexpected token when expecting an expression term or prefix operator: {current_token.type.name} ('{current_token.value}')",
                line_number=current_token.line, code_snippet=current_token.value
            ))
            next_index = i + 1 # Consume the unexpected token

        return result, next_index


    def _validate_expression(self, tokens: List[Token], start_index: int, min_precedence: int = -1) -> Tuple[ValidationResult, int]:
        """
        Recursively validates a NetLogo expression using Pratt parsing (Top-Down Operator Precedence).
        Handles infix operators based on precedence.

        Args:
            tokens: The list of tokens.
            start_index: The index in the token list where the expression starts.
            min_precedence: The minimum precedence level for operators to be consumed.

        Returns:
            A tuple containing:
            - ValidationResult: Indicates if the parsed expression is valid.
            - int: The index of the token immediately *after* the parsed expression.
        """
        result = ValidationResult(True)
        i = start_index

        # 1. Parse the left-hand side (prefix operators, primary terms)
        left_result, current_index = self._parse_prefix_or_primary(tokens, i)
        if not left_result.is_valid:
            # If the primary part is invalid, return immediately.
            return left_result, current_index

        # 2. Loop while the next token is an infix operator with sufficient precedence
        while True:
            if current_index >= len(tokens):
                break # End of input

            operator_token = tokens[current_index]
            current_precedence = self._get_token_precedence(operator_token)

            # Stop if token is not an operator or precedence is too low
            if current_precedence < min_precedence:
                break

            # --- Consume the operator ---
            current_index += 1 # Move past the operator

            # --- Parse the right-hand side ---
            # For binary operators, recursively call _validate_expression.
            # Precedence for the right operand depends on associativity.
            # For left-associative operators (+, -, *, /), parse right operand with precedence + 1.
            # For right-associative (^), parse right operand with the same precedence.
            # Assuming left-associativity for now for simplicity.
            # TODO: Handle right-associativity for '^' if needed.
            next_min_precedence = current_precedence + 1

            right_result, next_index = self._validate_expression(tokens, current_index, min_precedence=next_min_precedence)

            if not right_result.is_valid:
                # If the right operand is invalid, merge the error and stop parsing this expression part.
                # Add context about the operator.
                error_context = f"Invalid right-hand side for operator '{operator_token.value}' (line {operator_token.line})"
                if right_result.errors:
                     right_result.errors[0].message = f"{error_context}: {right_result.errors[0].message}"
                else: # Should not happen, but safeguard
                     right_result.add_error(ValidationError(error_context, line_number=operator_token.line))
                result.merge(right_result)
                return result, next_index # Return immediately on right-side error

            # --- Combine (Validation) ---
            # Both left and right operands are valid in structure.
            # We don't actually compute the result, just validate structure.
            # Merge results (currently just means is_valid remains true).
            result.merge(left_result) # Merge any previous errors/state
            result.merge(right_result)
            # Update the current index to after the right operand.
            current_index = next_index
            # The combined result becomes the new "left" for the next iteration (conceptually).
            left_result = result # Carry forward the combined valid state

        # Loop finished, return the combined result and the final index
        return result, current_index

    # --- Control Structure Validators (Tokenized - Use Pratt Parser) ---
    def _validate_if_statement(self, tokens: List[Token], start_idx: int) -> Tuple[ValidationResult, int]:
        """Validate if/ifelse/ifelse-value using tokens and the expression validator."""
        result = ValidationResult(True)
        i = start_idx
        statement_token = tokens[i]
        statement_type = statement_token.value.lower()

        # Handle multi-conditional format: (ifelse cond1 [...] cond2 [...] [...])
        is_multi_conditional = False
        if i > 0 and tokens[i-1].type == TokenType.LPAREN:
            is_multi_conditional = True
            # The multi-conditional validator expects to start *at* the LPAREN
            return self._validate_multi_conditional(tokens, i - 1, statement_type)

        # --- Standard if/ifelse ---
        i += 1 # Move past 'if'/'ifelse'

        if i >= len(tokens) or tokens[i].type == TokenType.EOF:
            result.add_error(ValidationError(f"Incomplete {statement_type} - missing condition", line_number=statement_token.line))
            return result, i

        # Validate condition expression using the Pratt parser, starting with lowest operator precedence.
        cond_result, cond_end_i = self._validate_expression(tokens, i, min_precedence=-1) # Use -1 base precedence
        if not cond_result.is_valid:
             result.merge(cond_result)
             # Attempt to recover if condition is invalid by finding the start of the true branch '['
             # This helps report errors in branches even if condition is bad.
             found_bracket = False
             temp_i = cond_end_i
             while temp_i < len(tokens) and tokens[temp_i].type != TokenType.LBRACKET and tokens[temp_i].type != TokenType.EOF:
                 temp_i += 1
             if temp_i < len(tokens) and tokens[temp_i].type == TokenType.LBRACKET:
                 i = temp_i # Recovered position
                 found_bracket = True
             else:
                 i = cond_end_i # Could not recover, proceed from end of invalid condition
                 # Add error if recovery failed and no bracket found immediately after invalid condition
                 if i < len(tokens) and tokens[i].type != TokenType.LBRACKET:
                      result.add_error(ValidationError(f"Expected '[' after invalid condition in {statement_type}", line_number=tokens[i-1].line))

        else: # Condition was valid
             i = cond_end_i

        # Expect true branch opening bracket
        if i >= len(tokens) or tokens[i].type != TokenType.LBRACKET:
            result.add_error(ValidationError(f"Expected '[' after condition in {statement_type}", line_number=tokens[i-1].line))
        else:
            i += 1 # Move past '['
            true_branch_start_i = i
            # Find matching RBRACKET
            bracket_level = 1
            while i < len(tokens):
                if tokens[i].type == TokenType.LBRACKET: bracket_level += 1
                elif tokens[i].type == TokenType.RBRACKET:
                    bracket_level -= 1
                    if bracket_level == 0: break
                elif tokens[i].type == TokenType.EOF: bracket_level = -1; break
                i += 1
            
            if bracket_level != 0:
                result.add_error(ValidationError(f"Unclosed bracket in {statement_type} true branch", line_number=tokens[true_branch_start_i-1].line))
                true_branch_end_i = i
            else:
                true_branch_end_i = i
                i += 1 # Move past ']'

            # Validate true branch contents
            true_branch_result = self._validate_branch_contents(tokens[true_branch_start_i:true_branch_end_i])
            if not true_branch_result.is_valid: result.merge(true_branch_result)

        # Handle false branch for ifelse/ifelse-value
        if statement_type in {'ifelse', 'ifelse-value'}:
            if i >= len(tokens) or tokens[i].type != TokenType.LBRACKET:
                result.add_error(ValidationError(f"Missing '[' for false branch in {statement_type}", line_number=tokens[i-1].line))
            else:
                i += 1 # Move past '['
                false_branch_start_i = i
                bracket_level = 1
                while i < len(tokens):
                    if tokens[i].type == TokenType.LBRACKET: bracket_level += 1
                    elif tokens[i].type == TokenType.RBRACKET:
                        bracket_level -= 1
                        if bracket_level == 0: break
                    elif tokens[i].type == TokenType.EOF: bracket_level = -1; break
                    i += 1

                if bracket_level != 0:
                    result.add_error(ValidationError(f"Unclosed bracket in {statement_type} false branch", line_number=tokens[false_branch_start_i-1].line))
                    false_branch_end_i = i
                else:
                    false_branch_end_i = i
                    i += 1 # Move past ']'

                # Validate false branch contents
                false_branch_result = self._validate_branch_contents(tokens[false_branch_start_i:false_branch_end_i])
                if not false_branch_result.is_valid: result.merge(false_branch_result)

        return result, i

    def _validate_multi_conditional(self, tokens: List[Token], start_idx: int, statement_type: str) -> Tuple[ValidationResult, int]:
        """Validate multi-conditional ifelse/ifelse-value using tokens."""
        result = ValidationResult(True)
        i = start_idx # Should start at LPAREN
        paren_token = tokens[i]

        if paren_token.type != TokenType.LPAREN:
             result.add_error(ValidationError(f"Expected '(' for multi-conditional {statement_type}", line_number=paren_token.line))
             return result, i + 1

        i += 1 # Move past '('

        if i >= len(tokens) or tokens[i].type != TokenType.COMMAND or tokens[i].value.lower() != statement_type:
             result.add_error(ValidationError(f"Expected '{statement_type}' after '('", line_number=paren_token.line))
             # Attempt recovery by finding RPAREN
             paren_level = 1
             while i < len(tokens):
                 if tokens[i].type == TokenType.LPAREN: paren_level += 1
                 elif tokens[i].type == TokenType.RPAREN:
                     paren_level -= 1
                     if paren_level == 0: break
                 elif tokens[i].type == TokenType.EOF: break
                 i += 1
             return result, i + 1 if i < len(tokens) else i
        i += 1 # Move past statement type (ifelse or ifelse-value)

        has_processed_at_least_one_pair = False
        while i < len(tokens):
            # --- Check for loop termination conditions first ---
            current_token = tokens[i]

            # 1. End of statement?
            if current_token.type == TokenType.RPAREN:
                i += 1 # Consume ')'
                break # Exit loop

            # 2. Optional final else block?
            if current_token.type == TokenType.LBRACKET:
                i += 1 # Consume '['
                else_branch_start_i = i
                bracket_level = 1
                while i < len(tokens):
                    if tokens[i].type == TokenType.LBRACKET: bracket_level += 1
                    elif tokens[i].type == TokenType.RBRACKET:
                        bracket_level -= 1
                        if bracket_level == 0: break
                    elif tokens[i].type == TokenType.EOF: bracket_level = -1; break
                    i += 1
                
                if bracket_level != 0:
                    result.add_error(ValidationError(f"Unclosed bracket in {statement_type} else branch", line_number=tokens[else_branch_start_i-1].line))
                    else_branch_end_i = i
                else:
                    else_branch_end_i = i
                    i += 1 # Move past ']'

                else_branch_result = self._validate_branch_contents(tokens[else_branch_start_i:else_branch_end_i])
                if not else_branch_result.is_valid: result.merge(else_branch_result)

                # After the final else branch, we MUST find the closing parenthesis
                if i >= len(tokens) or tokens[i].type != TokenType.RPAREN:
                     result.add_error(ValidationError(f"Expected ')' after final else branch in {statement_type}", line_number=tokens[i-1].line if i > 0 else paren_token.line))
                     # Don't consume if RPAREN is missing, let the final check handle it
                else:
                     i += 1 # Consume ')'
                break # Exit loop after processing final else

            # --- If not RPAREN or LBRACKET, assume it's a condition-branch pair ---
            has_processed_at_least_one_pair = True

            # 3. Parse Condition
            cond_result, cond_end_i = self._validate_expression(tokens, i, min_precedence=-1) # Use -1 base precedence
            if not cond_result.is_valid:
                 result.merge(cond_result)
                 # --- Let's simplify: If condition is invalid, stop processing this statement ---
                 i = cond_end_i # Advance index past the invalid expression
                 break # Exit the while loop

            # Condition was valid, update index
            i = cond_end_i

            # 4. Expect and Parse Command Block '[' ... ']'
            if i >= len(tokens) or tokens[i].type != TokenType.LBRACKET:
                 result.add_error(ValidationError(f"Expected '[' after condition in {statement_type}", line_number=tokens[i-1].line if i > 0 else paren_token.line))
                 # If '[' is missing, stop processing this statement
                 break # Exit the while loop
            else:
                 i += 1 # Move past '['
                 branch_start_i = i
                 bracket_level = 1
                 while i < len(tokens):
                     if tokens[i].type == TokenType.LBRACKET: bracket_level += 1
                     elif tokens[i].type == TokenType.RBRACKET:
                         bracket_level -= 1
                         if bracket_level == 0: break
                     elif tokens[i].type == TokenType.EOF: bracket_level = -1; break
                     i += 1

                 if bracket_level != 0:
                     result.add_error(ValidationError(f"Unclosed bracket in {statement_type} branch", line_number=tokens[branch_start_i-1].line))
                     branch_end_i = i
                     # Stop processing if branch is unclosed
                     break
                 else:
                     branch_end_i = i
                     i += 1 # Move past ']'

                 branch_result = self._validate_branch_contents(tokens[branch_start_i:branch_end_i])
                 if not branch_result.is_valid: result.merge(branch_result)
                 # Loop continues to check for RPAREN, LBRACKET, or next condition

        # --- Post-loop checks ---
        # Check if the loop exited because it reached the end of tokens unexpectedly
        if i < len(tokens) and tokens[i].type == TokenType.EOF and tokens[i-1].type != TokenType.RPAREN:
             result.add_error(ValidationError(f"Unexpected end of input within multi-conditional {statement_type}", line_number=tokens[i-1].line))
        # Check if we ended correctly (the token *before* the current index 'i' should be RPAREN)
        elif i == 0 or (i > 0 and tokens[i-1].type != TokenType.RPAREN):
             # Avoid adding duplicate error if already reported missing ')'
             if not result.errors or "Expected ')'" not in result.errors[-1].message:
                 line_num = tokens[i-1].line if i > 0 else paren_token.line
                 result.add_error(ValidationError(f"Expected ')' to end multi-conditional {statement_type}", line_number=line_num))
        # Check if any condition-branch pairs were processed
        elif not has_processed_at_least_one_pair:
             result.add_error(ValidationError(f"Multi-conditional {statement_type} must have at least one condition/branch pair", line_number=paren_token.line))


        return result, i # Return the index *after* the closing parenthesis (or where parsing stopped)

    def _validate_branch_contents(self, tokens: List[Token]) -> ValidationResult:
        """Validate the contents of an if/ifelse branch using tokens."""
        result = ValidationResult(True)
        if not tokens:
            # Empty branches are allowed, maybe add warning later if desired
            # result.add_error(ValidationError("Empty branch", severity=ErrorSeverity.WARNING))
            return result

        # Use the main syntax checker for the branch content
        branch_result = self._check_syntax_tokenized(tokens + [Token(TokenType.EOF, '', -1, -1)]) # Add EOF for checker
        if not branch_result.is_valid:
             result.merge(branch_result)

        return result

    def _validate_command(self, tokens: List[Token], start_idx: int) -> Tuple[ValidationResult, int]:
        """Validate a command and its arguments using tokens."""
        result = ValidationResult(True)
        i = start_idx
        command_token = tokens[i]
        command_lower = command_token.value.lower()

        if command_token.type != TokenType.COMMAND or command_lower not in self.allowed_commands:
             return result, i + 1 # Should not happen

        i += 1 # Move past command

        num_expected_args = 0
        if command_lower in {'fd', 'forward', 'bk', 'back', 'rt', 'right', 'lt', 'left'}:
            num_expected_args = 1
        elif command_lower in {'set', 'let'}:
            num_expected_args = 2

        consumed_args = 0
        while consumed_args < num_expected_args:
            if i >= len(tokens) or tokens[i].type == TokenType.EOF:
                 result.add_error(ValidationError(f"Command '{command_lower}' expects {num_expected_args} arg(s), found end", line_number=command_token.line))
                 break

            # Special handling for 'set'/'let' variable name (first argument)
            if command_lower in {'set', 'let'} and consumed_args == 0:
                 var_token = tokens[i]
                 if var_token.type != TokenType.IDENTIFIER:
                      result.add_error(ValidationError(f"Expected variable name after '{command_lower}', found {var_token.type.name}", line_number=var_token.line))
                 # Basic check for valid identifier format (already done by tokenizer, but good safeguard)
                 elif not re.match(r'^[a-zA-Z][\w\-]*$', var_token.value):
                      result.add_error(ValidationError(f"Invalid variable name format: '{var_token.value}'", line_number=var_token.line))
                 # Check if it's a disallowed primitive name (e.g., cannot 'set fd [...]')
                 elif var_token.value.lower() in self.allowed_commands or var_token.value.lower() in self.allowed_reporters:
                      result.add_error(ValidationError(f"Cannot use command/reporter name '{var_token.value}' as a variable", line_number=var_token.line))

                 # Consume only the variable name token
                 next_i = i + 1
            else:
                 # Validate the argument expression using the Pratt parser, starting with lowest operator precedence.
                 arg_result, next_i = self._validate_expression(tokens, i, min_precedence=-1) # Use -1 base precedence
                 if not arg_result.is_valid:
                     result.merge(arg_result)
                     # Add context to the error message
                     if result.errors: # Ensure there's an error to modify
                         result.errors[-1].message = f"Invalid arg {consumed_args + 1} for '{command_lower}': {result.errors[-1].message}"

            i = next_i
            consumed_args += 1

        return result, i

    # --- Removed Old/Redundant Expression/Condition Validators ---
    # The functionality of _is_valid_numeric_expression, _is_valid_reporter_expression,
    # _is_valid_complex_expression, _validate_condition, and _validate_simple_condition
    # is now handled comprehensively by the Pratt parser in _validate_expression.

    # --- Value Range Validator (Tokenized) ---
    def _check_value_ranges(self, tokens: List[Token]) -> ValidationResult:
        """
        Validate numeric value ranges using the token stream. Should be called after
        basic syntax validation to ensure tokens are meaningful.

        Args:
            tokens: The list of tokens representing the code.

        Returns:
            ValidationResult with any range violations.
        """
        result = ValidationResult(True)

        for token in tokens:
            if token.type == TokenType.NUMBER:
                num_str = token.value
                try:
                    value = float(num_str)
                    # Check against configured limits
                    if value > self.max_value:
                        result.add_error(ValidationError(
                            f"Value too large: {value} (maximum allowed: {self.max_value})",
                            line_number=token.line,
                            code_snippet=num_str
                        ))
                    if value < self.min_value:
                        result.add_error(ValidationError(
                            f"Value too small: {value} (minimum allowed: {self.min_value})",
                            line_number=token.line,
                            code_snippet=num_str
                        ))
                except ValueError:
                    # This shouldn't happen if the tokenizer is correct, but safeguard
                    result.add_error(ValidationError(
                        f"Invalid numeric token value: {num_str}",
                        line_number=token.line,
                        code_snippet=num_str
                    ))

        return result

    def measure_complexity(self, code: str) -> CodeComplexity:
        """
        Measure the complexity of NetLogo code.
        
        Args:
            code: The NetLogo code to analyze
            
        Returns:
            CodeComplexity: Enum value representing complexity level
        """
        # Clean the code
        code = re.sub(self.comment_pattern, '', code)
        code = code.lower()
        
        # Count various complexity indicators
        complexity_score = 0
        
        # 1. Basic movement commands (+1)
        if re.search(r'\b(fd|forward|bk|back|rt|right|lt|left)\b', code):
            complexity_score += 1
        
        # 2. Conditional statements (+1 per type)
        if re.search(r'\bif\b', code):
            complexity_score += 1
        if re.search(r'\bifelse\b', code):
            complexity_score += 1
        
        # 3. Variable usage (+1)
        if re.search(r'\b(set|let)\b', code):
            complexity_score += 1
        
        # 4. Advanced movement (+1)
        if re.search(r'\b(towards|distance|in-radius)\b', code):
            complexity_score += 1
        
        # 5. Random usage (+1)
        if re.search(r'\b(random|random-float)\b', code):
            complexity_score += 1
        
        # 6. Math functions (+1)
        if re.search(r'\b(sin|cos|tan)\b', code):
            complexity_score += 1
        
        # 7. Nested conditionals (+1)
        if re.search(r'\bif.*\[.*if\b', code) or re.search(r'\bifelse.*\[.*if\b', code):
            complexity_score += 1
        
        # Map score to complexity level, capping at the highest level
        if complexity_score <= 1:
            return CodeComplexity.SIMPLE
        elif complexity_score == 2:
            return CodeComplexity.BASIC
        elif complexity_score == 3:
            return CodeComplexity.MODERATE
        elif complexity_score == 4:
            return CodeComplexity.ADVANCED
        elif complexity_score == 5:
            return CodeComplexity.COMPLEX
        elif complexity_score == 6:
            return CodeComplexity.SOPHISTICATED
        else:
            return CodeComplexity.EXPERT

# Usage Example
def test_verifier():
    """Test the verifier with various inputs."""
    verifier = NetLogoVerifier()
    
    test_cases = [
        # Safe cases
        ("fd 0.5", True),
        ("fd random 10", True),
        ("rt random-float 90 fd 5", True),
        
        # Control structure cases
        ("ifelse item 0 input != 0 [rt 15 fd 0.5] [fd 1]", True),
        ("if item 1 input = 0 [lt 45 fd 1]", True),
        
        # Unsafe cases
        ("ask neighbors [fd 1]", False),
        ("fd 1 die", False),
        ("rt 90 python:run", False),
        ("fd (1", False),
        ("fd -9999", False),
        ("", False),
        
        # Edge cases
        ("fd 1 + 2", True),
        ("rt random 360 * 0.5", True),
        ("FD 1 RT 90", True),  # Case insensitive
        
        # Additional decimal test cases
        ("fd 0.25", True),
        ("rt 45.5", True),
        ("lt -0.75", True),
        
         # Complex control structure case
        ("""ifelse item 0 input != 0 [rt 15 fd 0.5] [ifelse item 2 input != 0 [fd 1] [ifelse item 1 input != 0 [lt 15 fd 0.5] [rt random 30 lt random 30 fd 5]]]""", True),
        # Corrected: Added missing closing parenthesis and whitespace for clarity (parser should handle lack of whitespace now)
        ("""(ifelse item 2 input != 0 [ fd 1 ] item 0 input != 0 and item 0 input < item 1 input [ lt 15 fd 0.5 ] item 1 input != 0 [ rt 15 fd 0.5 ] [ fd 1 ])""", True),

        ("lt random 20 rt random 20 fd (1 + random-float 0.5)", True),

        # Multi-conditional format test cases
        ("(ifelse item 0 input > 0 [fd 1] item 1 input > 0 [rt 90 fd 1] [lt 90 fd 1])", True),
        ("(ifelse-value item 0 input > 0 [1] item 1 input > 0 [2] [0])", True),
        # Corrected: Final else is optional, so this is valid
        ("(ifelse item 0 input > 0 [fd 1] item 1 input > 0 [rt 90 fd 1])", True),
        ("(ifelse item 0 input > 0 fd 1] item 1 input > 0 [rt 90 fd 1])", False),  # Syntax error (missing '[')
        ("(ifelse-value item 0 input > 0 [1 + 2] item 1 input > 0 [sin random 360] [0])", True),
        ("(ifelse item 0 input > 10 and item 1 input < 5 [fd 1] item 2 input != 0 [rt 45 fd 2] [lt 45 fd 1])", True), # Complex condition with logical operator
        # Corrected: Added missing closing parenthesis and whitespace
        ("(ifelse item 2 input != 0 [ fd 1 ] item 0 input != 0 and item 0 input < item 1 input [ lt 15 fd 0.5 ] item 1 input != 0 [ rt 15 fd 0.5 ] [ fd 1 ])", True),

        # --- New Test Cases for Pratt Parser ---
        # Operator Precedence
        ("fd 1 + 2 * 3", True), # 1 + (2*3) = 7
        ("fd 1 * 2 + 3", True), # (1*2) + 3 = 5
        ("fd 1 + 2 > 3 - 1", True), # (1+2) > (3-1) => 3 > 2
        # Parentheses
        ("fd (1 + 2) * 3", True), # (1+2)*3 = 9
        ("fd 1 + (2 * 3)", True), # 1 + (2*3) = 7 (Same as without parens)
        ("fd ((1 + 2))", True), # Extra parens
        # Unary Operators
        ("fd -xcor", True),
        ("fd -5", True),
        ("fd 1 + -5", True), # Binary plus, unary minus
        ("if not (xcor > 0) [ fd 1 ]", True),
        # Nested Reporters
        ("fd random (random 10)", True),
        ("fd item (random 2) (list 10 20)", True), # item needs 2 args
        # Logical Operators
        ("if xcor > 0 and ycor < 0 [ fd 1 ]", True),
        ("if xcor > 0 or ycor < 0 [ fd 1 ]", True),
        ("if (xcor > 0 and ycor < 0) or heading = 0 [ fd 1 ]", True),
        # Error Cases
        ("fd 1 +", False), # Missing right operand
        ("fd * 2", False), # Missing left operand for binary *
        ("fd -", False), # Missing operand for unary -
        ("fd random 10 20", False), # Too many args for random
        ("fd item 1", False), # Too few args for item
        ("fd (1 + 2", False), # Unclosed parenthesis
        ("fd 1 + )", False), # Unexpected closing parenthesis
        ("if 1 and [ fd 1 ]", False), # Incomplete condition
        ("set xcor", False), # Missing value for set
        ("set 1 10", False), # Invalid variable name for set
    ]

    all_passed = True
    for code, expected_safe in test_cases:
        is_safe, message = verifier.is_safe(code)
        print(f"\nTesting: {code}")
        print(f"Expected safe: {expected_safe}, Got: {is_safe}")
        if not is_safe:
            print(f"Message: {message}")
        assert is_safe == expected_safe, f"Test failed for: {code}"
        
        # Also test complexity measurement
        complexity = verifier.measure_complexity(code)
        print(f"Complexity: {complexity.name} ({complexity.value})")
        if is_safe != expected_safe:
            all_passed = False

    if all_passed:
        print("\n✅ All test cases passed!")
    else:
        print("\n❌ Some test cases failed.")

if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO) # Show logs during testing if needed
    test_verifier()